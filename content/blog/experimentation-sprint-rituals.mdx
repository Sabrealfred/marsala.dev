---
title: "Experimentation Rituals That Actually Scale"
slug: "experimentation-sprint-rituals"
type: "Playbook"
summary: "Rituals, tooling, and accountability loops that let our teams ship four experiments every sprint without chaos."
description: "Step-by-step guide to building an experimentation program with predictable intake, QA, telemetry, and retros."
date: "2024-11-22"
readingTime: "12 min read"
author: "Marsala Engineering Team"
tags:
  - "Growth"
  - "Product"
  - "Process"
keywords:
  - "experimentation"
  - "growth process"
  - "sprint rituals"
  - "feature flags"
featured: false
image: "/blog/experimentation.jpg"
---

# Experimentation Rituals That Actually Scale

## Background

Running experiments is easy; running **repeatable** experiments that teams trust is not. A year ago our squads were running one experiment per month, often without clean data or follow-up analysis. To double growth impact we overhauled the experimentation process—borrowing agile rituals, adding strong telemetry, and codifying ownership. We now average four experiments per sprint across web, messaging, and automation surfaces. This post shares how we did it.

## Operating Principles

1. **Hypotheses must be falsifiable and tied to a single metric.**
2. **Every experiment has an explicit owner** from ideation through debrief.
3. **Telemetry is non-negotiable.** No tracking plan, no launch.
4. **Feature flags everywhere.** Rollouts happen in minutes, not weeks.
5. **Document everything.** Wins, neutral results, and failures live in the same library.

## Tooling Overview

| Need | Tool | Notes |
|------|------|-------|
| Backlog & workflow | Linear | Dedicated “EX” team with templates for intake, QA, debrief |
| Telemetry | PostHog + dbt | Shared metrics layer, guardrails for sample size |
| Feature flags | PostHog flags + custom middleware | LaunchDarkly-level control without the price tag |
| Design review | Figma + Storybook | Components synced with tokens so UI changes mirror product |
| Communication | Resend summaries + #experiments Slack channel | Automated weekly recaps |

## Prioritization Rubric

We grade every proposal against the **FICE** rubric (Fit, Impact, Confidence, Effort). Each dimension is scored 1–5 with explicit prompts so debates stay objective:

* **Fit.** Does the experiment ladder up to the quarterly mission and the owning squad’s KPIs? We reject clever one-offs that don’t map to strategy.
* **Impact.** We estimate the size of the population touched and the expected lift using historical conversion bands. Analysts provide the model, not opinions.
* **Confidence.** Do we have directional evidence (qual insights, prior tests, user interviews) that justify the bet? High-confidence scores require linked artifacts.
* **Effort.** Engineering, design, and data sign off on the engineering hours plus QA cost. Anything above 5 days needs director approval.

The rubric lives directly in the Linear intake form so submitters know how proposals will be judged. It also makes trade-offs transparent when stakeholders ask why their idea waited a sprint.

## Rituals in Detail

### Weekly Intake (Monday)

Growth, product, and engineering meet for 30 minutes to review experiment proposals submitted via Linear. Each submission includes hypothesis, target metric, guardrail metrics, rollout plan, and expected engineering effort. We accept, defer, or reject proposals on the spot.

### Sprint Planning (Tuesday)

Accepted experiments receive:

* Assigned engineer + analyst.
* Design assets or copy requirements.
* Tracking plan (Segment events + dbt model updates).
* QA checklist covering browsers, devices, and accessibility.

We limit ourselves to four concurrent experiments per surface to avoid noise.

### Daily Standups

Each experiment owner gives a 60-second update: implementation status, blockers, telemetry readiness. If a blocker requires cross-team help, we escalate immediately rather than waiting for retro.

### Release + QA

* Feature flag created with default OFF.
* Playwright regression tests run automatically.
* Analytics QA: we use a custom script that fires PageView + custom events, then verifies they landed in PostHog/dbt within five minutes.
* Launch staged to 5% of traffic; health monitored for 30 minutes before scaling.

## Instrumentation & QA Checklist

Every experiment has a living checklist that must turn green before launch:

1. **Events defined.** Segment + PostHog schema reviewed by analytics, including naming conventions and guardrail metrics.
2. **Data contracts verified.** dbt tests run in CI to ensure new events don’t break downstream models.
3. **Accessibility + performance pass.** Lighthouse and axe CI checks must be ≥90; perf regressions require a compensating experiment.
4. **Rollback plan documented.** For each flag we sketch “what happens if we revert in the middle of onboarding” so CS and Sales aren’t surprised.
5. **Owner escalation tree.** If metrics swing beyond guardrails, everyone knows who pulls the plug and who communicates with leadership.

It sounds heavy, but the checklist sits next to the PR and takes five minutes to verify because the automation does most of the work.

### Friday Demo + Retro

We demo results (even if still running) and log learnings into Notion. Each entry includes:

* Link to code/flag.
* Screenshots/videos.
* Metric movement with confidence intervals.
* Decision (ship, iterate, revert, archive).

## Sample Timeline

| Day | Activity |
|-----|----------|
| Monday | Intake + prioritization |
| Tuesday | Planning + instrumentation reviews |
| Wednesday | Build + QA |
| Thursday | Launch + monitoring |
| Friday | Demo + retro + backlog grooming |

## Org Enablement and Incentives

Process dies when incentives clash. We invited marketing, success, and sales ops to nominate “experiment buddies” who shadow a squad for two sprints. Buddies help write hypotheses, sanity-check guardrails, and carry learnings back to their teams. Leadership reviews a one-page “experiments shipped vs. learnings captured” scorecard in the weekly business review, so executives celebrate disciplined invalidations rather than only wins. We also track experiment debt (tests stuck without analysis) and treat it like tech debt—if the number creeps above three, we pause new launches until analysis catches up.

## Scaling to Multiple Squads

Once two squads adopted the rituals, we packaged them into a starter kit:

* **Notion workspace template** with linked databases for hypotheses, rollouts, and retros.
* **Linear project template** that auto-generates the right statuses, custom fields, and automation rules.
* **Starter telemetry pack** (dbt macros + dashboard) so new squads don’t reinvent charts.
* **Buddy rotation calendar** assigning an analyst and designer to each new squad for their first sprint.

We launch new squads in “mentored mode” for two iterations. After that they enter the shared operating rhythm and contribute to the weekly backlog refinement.

## Metrics We Track

* **Experiments launched per sprint:** 4 (goal 3–5).
* **Analysis completion rate:** 100% (no open loops).
* **Average time to decision:** 6.2 days.
* **Experiment win rate:** 27% (rest either neutral or instructive failures).
* **Technical rollback rate:** &lt;2% thanks to feature flags and QA.

## Templates and Documentation

Our Linear template includes sections for hypothesis, design artifacts, tracking plan, QA checklist, flag configuration, and postmortem summary. Notion serves as an indexed knowledge base; we tag experiments by surface, metric, and outcome so future teams can learn quickly. Resend automates weekly digests summarizing what shipped, what’s mid-flight, and what we learned.

## Lessons Learned

* **Enforce guardrails.** Sample-size calculators and stopping rules are embedded into our dashboards. If an experiment tries to end early, the analyst gets paged.
* **Tie experiments to north-star metrics.** Vanity tests are an easy trap; we require explicit links to OKRs.
* **Share failures loudly.** The best ideas often reseed from what didn’t work. Transparency keeps the culture healthy.
* **Make QA everyone’s job.** Designers review visuals, analysts verify events, engineers ensure performance, PMs sanity-check UX.

## FAQ

**How do you avoid experiment collisions?**  
We maintain a shared calendar inside Notion showing which cohorts are exposed to which flags. PostHog cohorts are tagged with experiment IDs, and middleware blocks conflicting flags from activating simultaneously.

**What about long-running experiments?**  
Anything exceeding three sprints requires a special review where we revalidate assumptions and consider alternative designs (like staggered rollouts or synthetic control groups).

**Do stakeholders outside engineering access results?**  
Yes. Resend digests include executive summaries plus links to dashboards. We also host a monthly “growth show-and-tell” where product marketing, success, and sales learn from experiment outcomes.

## What’s Next

We’re piloting auto-generated hypotheses using telemetry + LLMs to help teams spot underexplored segments. We’re also adding automated debrief summaries so insights land in Slack minutes after an experiment wraps. If you want the Linear template, tracking plan checklist, or Resend digest snippet, [reach out](/contact) and we’ll send the package.
